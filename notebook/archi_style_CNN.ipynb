{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b420dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65925d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Auto-reload extensions\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Display Matplotlib plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9483dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load the data from zipfile locally#####\n",
    "\n",
    "# import zipfile\n",
    "\n",
    "# zip_file_path = os.path.join('..', 'raw_data', 'archive.zip')\n",
    "# extract_dir = os.path.join('..', 'raw_data', 'cnn_data')\n",
    "\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_dir)\n",
    "\n",
    "# extracted_files = os.listdir(extract_dir)\n",
    "# print(\"Extracted files:\", extracted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2123953",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "1. remove the Grayscale images\n",
    "2. resize images\n",
    "3. normalization\n",
    "4. argumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f546e",
   "metadata": {},
   "source": [
    "## Remove Grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c711af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = [\n",
    "    'Achaemenid architecture',\n",
    "    'American craftsman style',\n",
    "    'American Foursquare architecture',\n",
    "    'Ancient Egyptian architecture',\n",
    "    'Art Deco architecture',\n",
    "    'Art Nouveau architecture',\n",
    "    'Baroque architecture',\n",
    "    'Bauhaus architecture',\n",
    "    'Beaux-Arts architecture',\n",
    "    'Byzantine architecture',\n",
    "    'Chicago school architecture',\n",
    "    'Colonial architecture',\n",
    "    'Deconstructivism',\n",
    "    'Edwardian architecture',\n",
    "    'Georgian architecture',\n",
    "    'Gothic architecture',\n",
    "    'Greek Revival architecture',\n",
    "    'International style',\n",
    "    'Novelty architecture',\n",
    "    'Palladian architecture',\n",
    "    'Postmodern architecture',\n",
    "    'Queen Anne architecture',\n",
    "    'Romanesque architecture',\n",
    "    'Russian Revival architecture',\n",
    "    'Tudor Revival architecture'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e089a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "base_folder_path = '../raw_data/cnn_data/architectural-styles-dataset'\n",
    "\n",
    "def black_white(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    if np.all(image_array[:, :, 0] == image_array[:, :, 1]) and  np.all(image_array[:, :, 1]== image_array[:, :, 2]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_images_in_style(style_name):\n",
    "    folder_path = os.path.join(base_folder_path, style_name)\n",
    "    deleted_count = 0\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, file_name)\n",
    "        if black_white(image_path):\n",
    "            os.remove(image_path)\n",
    "            deleted_count += 1\n",
    "#             print(f\"Grayscale image found: {file_name} in {style_name}\")\n",
    "\n",
    "    print(f\"Deleted {deleted_count} files in the {style_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9c4f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 files in the Art Nouveau architecture\n",
      "Processing images for style: Baroque architecture\n",
      "Deleted 0 files in the Baroque architecture\n",
      "Processing images for style: Bauhaus architecture\n",
      "Deleted 0 files in the Bauhaus architecture\n",
      "Processing images for style: Beaux-Arts architecture\n",
      "Deleted 0 files in the Beaux-Arts architecture\n",
      "Processing images for style: Byzantine architecture\n",
      "Deleted 0 files in the Byzantine architecture\n",
      "Processing images for style: Chicago school architecture\n",
      "Deleted 0 files in the Chicago school architecture\n",
      "Processing images for style: Colonial architecture\n",
      "Deleted 0 files in the Colonial architecture\n",
      "Processing images for style: Deconstructivism\n",
      "Deleted 0 files in the Deconstructivism\n",
      "Processing images for style: Edwardian architecture\n",
      "Deleted 0 files in the Edwardian architecture\n",
      "Processing images for style: Georgian architecture\n",
      "Deleted 0 files in the Georgian architecture\n",
      "Processing images for style: Gothic architecture\n",
      "Deleted 0 files in the Gothic architecture\n",
      "Processing images for style: Greek Revival architecture\n",
      "Deleted 0 files in the Greek Revival architecture\n",
      "Processing images for style: International style\n",
      "Deleted 0 files in the International style\n",
      "Processing images for style: Novelty architecture\n",
      "Deleted 0 files in the Novelty architecture\n",
      "Processing images for style: Palladian architecture\n",
      "Deleted 0 files in the Palladian architecture\n",
      "Processing images for style: Postmodern architecture\n",
      "Deleted 0 files in the Postmodern architecture\n",
      "Processing images for style: Queen Anne architecture\n",
      "Deleted 0 files in the Queen Anne architecture\n",
      "Processing images for style: Romanesque architecture\n",
      "Deleted 0 files in the Romanesque architecture\n",
      "Processing images for style: Russian Revival architecture\n",
      "Deleted 0 files in the Russian Revival architecture\n",
      "Processing images for style: Tudor Revival architecture\n",
      "Deleted 0 files in the Tudor Revival architecture\n"
     ]
    }
   ],
   "source": [
    "# Process images for each folder\n",
    "for style in styles:\n",
    "    print(f\"Processing images for style: {style}\")\n",
    "    process_images_in_style(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49998f2f",
   "metadata": {},
   "source": [
    "## Split the dataset into Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c6aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Achaemenid architecture: 273 train, 58 val, 59 test\n",
      "Processed American craftsman style: 254 train, 54 val, 56 test\n",
      "Processed American Foursquare architecture: 251 train, 53 val, 55 test\n",
      "Processed Ancient Egyptian architecture: 284 train, 60 val, 62 test\n",
      "Processed Art Deco architecture: 389 train, 83 val, 84 test\n",
      "Processed Art Nouveau architecture: 424 train, 91 val, 92 test\n",
      "Processed Baroque architecture: 317 train, 68 val, 69 test\n",
      "Processed Bauhaus architecture: 201 train, 43 val, 44 test\n",
      "Processed Beaux-Arts architecture: 291 train, 62 val, 64 test\n",
      "Processed Byzantine architecture: 216 train, 46 val, 47 test\n",
      "Processed Chicago school architecture: 169 train, 36 val, 37 test\n",
      "Processed Colonial architecture: 326 train, 70 val, 71 test\n",
      "Processed Deconstructivism: 233 train, 49 val, 51 test\n",
      "Processed Edwardian architecture: 195 train, 41 val, 43 test\n",
      "Processed Georgian architecture: 264 train, 56 val, 58 test\n",
      "Processed Gothic architecture: 227 train, 48 val, 50 test\n",
      "Processed Greek Revival architecture: 343 train, 73 val, 75 test\n",
      "Processed International style: 277 train, 59 val, 60 test\n",
      "Processed Novelty architecture: 260 train, 55 val, 57 test\n",
      "Processed Palladian architecture: 235 train, 50 val, 52 test\n",
      "Processed Postmodern architecture: 225 train, 48 val, 49 test\n",
      "Processed Queen Anne architecture: 497 train, 106 val, 108 test\n",
      "Processed Romanesque architecture: 207 train, 44 val, 46 test\n",
      "Processed Russian Revival architecture: 245 train, 52 val, 54 test\n",
      "Processed Tudor Revival architecture: 315 train, 67 val, 68 test\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define the base directory where the data is currently located\n",
    "base_dir = '../raw_data/cnn_data/architectural-styles-dataset'\n",
    "# Define the new base directory where the train/val/test folders will be created\n",
    "new_base_dir = '../raw_data/cnn_data'\n",
    "\n",
    "# Create directories for train, val, and test sets\n",
    "train_dir = os.path.join(new_base_dir, 'train')\n",
    "val_dir = os.path.join(new_base_dir, 'val')\n",
    "test_dir = os.path.join(new_base_dir, 'test')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Function to split and copy files\n",
    "def split_and_copy_files(style_name):\n",
    "    source_folder = os.path.join(base_dir, style_name)\n",
    "    if not os.path.exists(source_folder):\n",
    "        print(f\"Folder not found: {source_folder}\")\n",
    "        return\n",
    "\n",
    "    files = os.listdir(source_folder)\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # Calculate split indices\n",
    "    total_files = len(files)\n",
    "    train_count = int(0.7 * total_files)\n",
    "    val_count = int(0.15 * total_files)\n",
    "\n",
    "    train_files = files[:train_count]\n",
    "    val_files = files[train_count:train_count + val_count]\n",
    "    test_files = files[train_count + val_count:]\n",
    "\n",
    "    # Create style-specific directories in train, val, and test folders\n",
    "    train_style_dir = os.path.join(train_dir, style_name)\n",
    "    val_style_dir = os.path.join(val_dir, style_name)\n",
    "    test_style_dir = os.path.join(test_dir, style_name)\n",
    "\n",
    "    os.makedirs(train_style_dir, exist_ok=True)\n",
    "    os.makedirs(val_style_dir, exist_ok=True)\n",
    "    os.makedirs(test_style_dir, exist_ok=True)\n",
    "\n",
    "    # Copy files to respective directories\n",
    "    for file_name in train_files:\n",
    "        shutil.copy(os.path.join(source_folder, file_name), os.path.join(train_style_dir, file_name))\n",
    "\n",
    "    for file_name in val_files:\n",
    "        shutil.copy(os.path.join(source_folder, file_name), os.path.join(val_style_dir, file_name))\n",
    "\n",
    "    for file_name in test_files:\n",
    "        shutil.copy(os.path.join(source_folder, file_name), os.path.join(test_style_dir, file_name))\n",
    "\n",
    "    print(f\"Processed {style_name}: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
    "\n",
    "\n",
    "# Split and copy files for each style\n",
    "for style in styles:\n",
    "    split_and_copy_files(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d99c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ae68d6e",
   "metadata": {},
   "source": [
    "## Resize the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1754f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6918 files belonging to 25 classes.\n",
      "Found 1472 files belonging to 25 classes.\n",
      "Found 1511 files belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  labels = \"inferred\",\n",
    "  label_mode = \"int\",\n",
    "  seed=123,\n",
    "  image_size=(150, 150),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# We define a second one for the test data\n",
    "val_ds = image_dataset_from_directory(\n",
    "  val_dir,\n",
    "  labels = \"inferred\",\n",
    "  label_mode = \"int\",\n",
    "  seed=123,\n",
    "  image_size=(150, 150),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "  test_dir,\n",
    "  labels = \"inferred\",\n",
    "  label_mode = \"int\",\n",
    "  seed=123,\n",
    "  image_size=(150, 150),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926fb7db",
   "metadata": {},
   "source": [
    "## Data Normalize and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0952f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Rescaling, RandomFlip, RandomRotation, RandomZoom, RandomTranslation\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers, optimizers, callbacks\n",
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "# Define data augmentation layers\n",
    "data_augmentation = Sequential([\n",
    "    RandomFlip(\"horizontal_and_vertical\"),\n",
    "    RandomRotation(0.1),\n",
    "    RandomZoom(0.1),\n",
    "    RandomTranslation(0.2, 0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9124d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "normalization_layer = Rescaling(1./255)\n",
    "\n",
    "# Apply the normalization layer to the datasets\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(normalization_layer(x)), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Prefetch the datasets for better performance\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9b7a9",
   "metadata": {},
   "source": [
    "# Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6e314ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import layers, optimizers, callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax[0].set_title('loss')\n",
    "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    ax[1].set_title('accuracy')\n",
    "    ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Train acc\")\n",
    "    ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation acc\")\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e88e17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_history(history, name_history, history_1, name_history_1):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "    ax[0].set_title('loss')\n",
    "\n",
    "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss \" + name_history)\n",
    "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss \" + name_history)\n",
    "\n",
    "    ax[0].plot(history_1.epoch, history_1.history[\"loss\"], label=\"Train loss \" + name_history_1)\n",
    "    ax[0].plot(history_1.epoch, history_1.history[\"val_loss\"], label=\"Validation loss \" + name_history_1)\n",
    "\n",
    "    ax[1].set_title('Accuracy')\n",
    "\n",
    "    ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Train Accuracy \" + name_history)\n",
    "    ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation Accuracy \" + name_history)\n",
    "\n",
    "    ax[1].plot(history_1.epoch, history_1.history[\"accuracy\"], label=\"Train Accuracy \" + name_history_1)\n",
    "    ax[1].plot(history_1.epoch, history_1.history[\"val_accuracy\"], label=\"Validation Accuracy \" + name_history_1)\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3555f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8767e9e",
   "metadata": {},
   "source": [
    "## 1. VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34d40332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "base_model = VGG16(weights = \"imagenet\", include_top = False, input_shape = (150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "642434db",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape = (150, 150, 3))\n",
    "\n",
    "x = preprocess_input(inputs) # Then a preprocessing layer specifically designed for the VGG16\n",
    "x = base_model(x) # Then our transfer learning model\n",
    "\n",
    "x = layers.Flatten()(x) # Followed by our custom dense layers, tailored to our binary task\n",
    "\n",
    "x = layers.Dense(128, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(64, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "pred = layers.Dense(25, activation = \"softmax\")(x)\n",
    "\n",
    "# We use the keras Functional API to create our keras model\n",
    "\n",
    "model_vgg = Model(inputs = inputs , outputs = pred)\n",
    "\n",
    "# And we freeze the VGG16 model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27d37d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem_1   (None, 150, 150, 3)      0         \n",
      " (SlicingOpLambda)                                               \n",
      "                                                                 \n",
      " tf.nn.bias_add_1 (TFOpLambd  (None, 150, 150, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               1048704   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 25)                1625      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,773,273\n",
      "Trainable params: 1,058,585\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "066dbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(learning_rate = 0.001)\n",
    "model_vgg.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6ac00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"model_vgg\"\n",
    "\n",
    "modelCheckpoint = callbacks.ModelCheckpoint(\"{}.h5\".format(MODEL), monitor=\"val_loss\", verbose=0, save_best_only=True)\n",
    "\n",
    "LRreducer = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor = 0.1, patience=3, verbose=1, min_lr=0)\n",
    "\n",
    "EarlyStopper = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2e8aecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "217/217 [==============================] - 2092s 10s/step - loss: 4.1740 - accuracy: 0.1065 - val_loss: 2.8855 - val_accuracy: 0.1990 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "114/217 [==============>...............] - ETA: 31:15 - loss: 3.0991 - accuracy: 0.1546"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history_vgg = model_vgg.fit(\n",
    "        train_ds,\n",
    "        epochs=20,\n",
    "        validation_data=val_ds,\n",
    "        callbacks = [modelCheckpoint, LRreducer, EarlyStopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c03b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd0b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aef7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
