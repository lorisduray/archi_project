{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b420dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65925d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Auto-reload extensions\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Display Matplotlib plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9483dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load the data from zipfile locally#####\n",
    "\n",
    "# import zipfile\n",
    "\n",
    "# zip_file_path = os.path.join('..', 'raw_data', 'archive.zip')\n",
    "# extract_dir = os.path.join('..', 'raw_data', 'cnn_data')\n",
    "\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_dir)\n",
    "\n",
    "# extracted_files = os.listdir(extract_dir)\n",
    "# print(\"Extracted files:\", extracted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2123953",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "1. remove the Grayscale images\n",
    "2. resize images\n",
    "3. normalization\n",
    "4. argumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f546e",
   "metadata": {},
   "source": [
    "## Remove Grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c711af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = [\n",
    "    'Achaemenid architecture',\n",
    "    'American craftsman style',\n",
    "    'American Foursquare architecture',\n",
    "    'Ancient Egyptian architecture',\n",
    "    'Art Deco architecture',\n",
    "    'Art Nouveau architecture',\n",
    "    'Baroque architecture',\n",
    "    'Bauhaus architecture',\n",
    "    'Beaux-Arts architecture',\n",
    "    'Byzantine architecture',\n",
    "    'Chicago school architecture',\n",
    "    'Colonial architecture',\n",
    "    'Deconstructivism',\n",
    "    'Edwardian architecture',\n",
    "    'Georgian architecture',\n",
    "    'Gothic architecture',\n",
    "    'Greek Revival architecture',\n",
    "    'International style',\n",
    "    'Novelty architecture',\n",
    "    'Palladian architecture',\n",
    "    'Postmodern architecture',\n",
    "    'Queen Anne architecture',\n",
    "    'Romanesque architecture',\n",
    "    'Russian Revival architecture',\n",
    "    'Tudor Revival architecture'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "base_folder_path = '../raw_data/cnn_data/architectural-styles-dataset'\n",
    "\n",
    "def black_white(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    if np.all(image_array[:, :, 0] == image_array[:, :, 1]) and  np.all(image_array[:, :, 1]== image_array[:, :, 2]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_images_in_style(style_name):\n",
    "    folder_path = os.path.join(base_folder_path, style_name)\n",
    "    deleted_count = 0\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, file_name)\n",
    "        if black_white(image_path):\n",
    "            os.remove(image_path)\n",
    "            deleted_count += 1\n",
    "#             print(f\"Grayscale image found: {file_name} in {style_name}\")\n",
    "\n",
    "    print(f\"Deleted {deleted_count} files in the {style_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images for each folder\n",
    "for style in styles:\n",
    "    print(f\"Processing images for style: {style}\")\n",
    "    process_images_in_style(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49998f2f",
   "metadata": {},
   "source": [
    "## Split the dataset into Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define the base directory where the data is currently located\n",
    "base_dir = '../raw_data/cnn_data/architectural-styles-dataset'\n",
    "# Define the new base directory where the train/val/test folders will be created\n",
    "new_base_dir = '../raw_data/cnn_data'\n",
    "\n",
    "# Create directories for train, val, and test sets\n",
    "train_dir = os.path.join(new_base_dir, 'train')\n",
    "val_dir = os.path.join(new_base_dir, 'val')\n",
    "test_dir = os.path.join(new_base_dir, 'test')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Function to split and copy files\n",
    "def split_and_copy_files(style_name):\n",
    "    source_folder = os.path.join(base_dir, style_name)\n",
    "    if not os.path.exists(source_folder):\n",
    "        print(f\"Folder not found: {source_folder}\")\n",
    "        return\n",
    "\n",
    "    files = os.listdir(source_folder)\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # Calculate split indices\n",
    "    total_files = len(files)\n",
    "    train_count = int(0.7 * total_files)\n",
    "    val_count = int(0.15 * total_files)\n",
    "\n",
    "    train_files = files[:train_count]\n",
    "    val_files = files[train_count:train_count + val_count]\n",
    "    test_files = files[train_count + val_count:]\n",
    "\n",
    "    # Create style-specific directories in train, val, and test folders\n",
    "    train_style_dir = os.path.join(train_dir, style_name)\n",
    "    val_style_dir = os.path.join(val_dir, style_name)\n",
    "    test_style_dir = os.path.join(test_dir, style_name)\n",
    "\n",
    "    os.makedirs(train_style_dir, exist_ok=True)\n",
    "    os.makedirs(val_style_dir, exist_ok=True)\n",
    "    os.makedirs(test_style_dir, exist_ok=True)\n",
    "\n",
    "    # Copy files to respective directories\n",
    "    for file_name in train_files:\n",
    "        shutil.copy(os.path.join(source_folder, file_name), os.path.join(train_style_dir, file_name))\n",
    "\n",
    "    for file_name in val_files:\n",
    "        shutil.copy(os.path.join(source_folder, file_name), os.path.join(val_style_dir, file_name))\n",
    "\n",
    "    for file_name in test_files:\n",
    "        shutil.copy(os.path.join(source_folder, file_name), os.path.join(test_style_dir, file_name))\n",
    "\n",
    "    print(f\"Processed {style_name}: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
    "\n",
    "\n",
    "# Split and copy files for each style\n",
    "for style in styles:\n",
    "    split_and_copy_files(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf2d5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define the base directory where the data is currently located\n",
    "base_dir = '../raw_data/cnn_data/architectural-styles-dataset'\n",
    "# Define the new base directory where the train/val/test folders will be created\n",
    "new_base_dir = '../raw_data/cnn_data'\n",
    "\n",
    "# Create directories for train, val, and test sets\n",
    "train_dir = os.path.join(new_base_dir, 'train')\n",
    "val_dir = os.path.join(new_base_dir, 'val')\n",
    "test_dir = os.path.join(new_base_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae68d6e",
   "metadata": {},
   "source": [
    "## Resize the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1754f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6918 files belonging to 25 classes.\n",
      "Found 1472 files belonging to 25 classes.\n",
      "Found 1511 files belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  labels = \"inferred\",\n",
    "  label_mode = \"int\",\n",
    "  seed=123,\n",
    "  image_size=(150, 150),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# We define a second one for the test data\n",
    "val_ds = image_dataset_from_directory(\n",
    "  val_dir,\n",
    "  labels = \"inferred\",\n",
    "  label_mode = \"int\",\n",
    "  seed=123,\n",
    "  image_size=(150, 150),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "  test_dir,\n",
    "  labels = \"inferred\",\n",
    "  label_mode = \"int\",\n",
    "  seed=123,\n",
    "  image_size=(150, 150),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6b92edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Achaemenid architecture', 'American Foursquare architecture', 'American craftsman style', 'Ancient Egyptian architecture', 'Art Deco architecture', 'Art Nouveau architecture', 'Baroque architecture', 'Bauhaus architecture', 'Beaux-Arts architecture', 'Byzantine architecture', 'Chicago school architecture', 'Colonial architecture', 'Deconstructivism', 'Edwardian architecture', 'Georgian architecture', 'Gothic architecture', 'Greek Revival architecture', 'International style', 'Novelty architecture', 'Palladian architecture', 'Postmodern architecture', 'Queen Anne architecture', 'Romanesque architecture', 'Russian Revival architecture', 'Tudor Revival architecture']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3af42ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926fb7db",
   "metadata": {},
   "source": [
    "## Data Normalize and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0952f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Rescaling, RandomFlip, RandomRotation, RandomZoom, RandomTranslation\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers, optimizers, callbacks\n",
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "# Define data augmentation layers\n",
    "data_augmentation = Sequential([\n",
    "    RandomFlip(\"horizontal_and_vertical\"),\n",
    "    RandomRotation(0.1),\n",
    "    RandomZoom(0.1),\n",
    "    RandomTranslation(0.2, 0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9124d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "normalization_layer = Rescaling(1./255)\n",
    "\n",
    "# Apply the normalization layer to the datasets\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(normalization_layer(x)), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Prefetch the datasets for better performance\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4baaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9b7a9",
   "metadata": {},
   "source": [
    "# Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6e314ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import layers, optimizers, callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax[0].set_title('loss')\n",
    "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    ax[1].set_title('accuracy')\n",
    "    ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Train acc\")\n",
    "    ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation acc\")\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e88e17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_history(history, name_history, history_1, name_history_1):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "    ax[0].set_title('loss')\n",
    "\n",
    "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss \" + name_history)\n",
    "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss \" + name_history)\n",
    "\n",
    "    ax[0].plot(history_1.epoch, history_1.history[\"loss\"], label=\"Train loss \" + name_history_1)\n",
    "    ax[0].plot(history_1.epoch, history_1.history[\"val_loss\"], label=\"Validation loss \" + name_history_1)\n",
    "\n",
    "    ax[1].set_title('Accuracy')\n",
    "\n",
    "    ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Train Accuracy \" + name_history)\n",
    "    ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation Accuracy \" + name_history)\n",
    "\n",
    "    ax[1].plot(history_1.epoch, history_1.history[\"accuracy\"], label=\"Train Accuracy \" + name_history_1)\n",
    "    ax[1].plot(history_1.epoch, history_1.history[\"val_accuracy\"], label=\"Validation Accuracy \" + name_history_1)\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3555f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8767e9e",
   "metadata": {},
   "source": [
    "## 1. VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34d40332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "base_model = VGG16(weights = \"imagenet\", include_top = False, input_shape = (150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "642434db",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape = (150, 150, 3))\n",
    "\n",
    "# x = data_augmentation(inputs)\n",
    "# x = layers.Rescaling(1./255)(x)\n",
    "x = preprocess_input(inputs) # Then a preprocessing layer specifically designed for the VGG16\n",
    "x = base_model(x) # Then our transfer learning model\n",
    "\n",
    "x = layers.Flatten()(x) # Followed by our custom dense layers, tailored to our binary task\n",
    "\n",
    "x = layers.Dense(64, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(32, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "pred = layers.Dense(25, activation = \"softmax\")(x)\n",
    "\n",
    "# We use the keras Functional API to create our keras model\n",
    "\n",
    "model_vgg = Model(inputs = inputs , outputs = pred)\n",
    "\n",
    "# And we freeze the VGG16 model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27d37d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem_5   (None, 150, 150, 3)      0         \n",
      " (SlicingOpLambda)                                               \n",
      "                                                                 \n",
      " tf.nn.bias_add_5 (TFOpLambd  (None, 150, 150, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                524352    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 25)                825       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,241,945\n",
      "Trainable params: 527,257\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "066dbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(learning_rate = 0.001)\n",
    "model_vgg.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a6ac00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"model_vgg\"\n",
    "\n",
    "modelCheckpoint = callbacks.ModelCheckpoint(\"{}.h5\".format(MODEL), monitor=\"val_loss\", verbose=0, save_best_only=True)\n",
    "\n",
    "LRreducer = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor = 0.1, patience=3, verbose=1, min_lr=0)\n",
    "\n",
    "EarlyStopper = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a2e8aecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  1/217 [..............................] - ETA: 40:43 - loss: 5.1619 - accuracy: 0.0312"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history_vgg = model_vgg.fit(\n",
    "        train_ds,\n",
    "        epochs=10,\n",
    "        validation_data=val_ds,\n",
    "        callbacks = [modelCheckpoint, LRreducer, EarlyStopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "63ee9da1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_vgg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_history(\u001b[43mhistory_vgg\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_vgg' is not defined"
     ]
    }
   ],
   "source": [
    "plot_history(history_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aef7e2",
   "metadata": {},
   "source": [
    "## 2. CS231N Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4b0b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "\n",
    "def create_cs231n_model(input_shape=(150, 150, 3), num_classes=25):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional Layer\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Third Convolutional Layer\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c474c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape and number of classes\n",
    "input_shape = (150, 150, 3)\n",
    "num_classes = 25\n",
    "\n",
    "model_cs231n  = create_cs231n_model(input_shape , num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14a42c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 36992)             0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               4735104   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 25)                3225      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,831,577\n",
      "Trainable params: 4,831,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cs231n.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "762d38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(learning_rate = 0.001)\n",
    "model_cs231n.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c826c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCheckpoint = callbacks.ModelCheckpoint(\"model_cs231n.h5\", monitor=\"val_loss\", verbose=0, save_best_only=True)\n",
    "\n",
    "LRreducer = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor = 0.1, patience=3, verbose=1, min_lr=0)\n",
    "\n",
    "EarlyStopper = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1874560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 25) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history_cs231n \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cs231n\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodelCheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLRreducer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEarlyStopper\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filee41dbj_7.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/abbywang/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 25) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history_cs231n = model_cs231n.fit(\n",
    "        train_ds,\n",
    "        epochs=20,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[modelCheckpoint, LRreducer, EarlyStopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f60ce8ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_cs231n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_history(\u001b[43mhistory_cs231n\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_cs231n' is not defined"
     ]
    }
   ],
   "source": [
    "plot_history(history_cs231n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
